Spark RAPIDS Accelerator --> for Spark to use GPU computing

Python bindings are interfaces that allow Python code to interact with libraries written in other programming languages, such as C or C++

Q)  If pyspark is just a API library whose actual executables are present in APACHE SPARK BINARIES , then how is pyspark
    working properly without installing APACHE SPARK.
A)  I think pyspark does come with light weight SPARK BINARIES enough to execute PYSPARK APIS without any 
    cluster management part 


Clean later -->> Spark analogy pt in this file is 7 MB try reducing the size later /home/yvm/Documents/y_youtube/y_youtube_teaching/y_pyspark/y_pyspark_batch.md