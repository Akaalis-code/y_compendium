from pyspark.sql import sparksession
from pyspark.sql.functions import *
from pyspark.sql.window import Window


ss = SparkSession.builder.getOrCreate()
df1 = ss.read.option(header=True).csv("path")

df2 = df1.groupBy("col1" , "col2").sum("col3").show()


df3 = df1.withColumn("rank" , rank("col-n").over(Window.partitionBy("col-1" , "col-2").orderBy("col-3")))