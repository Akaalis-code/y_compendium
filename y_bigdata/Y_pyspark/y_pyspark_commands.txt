############ Install PYSPARK and find out where the library is installed in  ## START ###################################################################

1) Install pyspark using terminal
    pip install pyspark 

2) In case you want to see code inside the library and want to open its installed locations , run below python code
    import pyspark
    fp=pyspark.__file__
    print(fp)

3) Find out the "pyspark WEBUI" URL for your sparksessions

    After your sparkContext or SparkSession is running , execute below command inside your python terminal
        from pyspark.sql import SparkSession
        ss=SparkSession.builder.getOrCreate()
        ss.sparkContext.uiWebUrl

############ Install PYSPARK and find out where the library is installed in ## END ###################################################################






##################### DATA SETUP ## START ###################################################################
 
    from pyspark.sql import SparkSession

    ss=SparkSession.builder.getOrCreate()
    students_df=ss.createDataFrame( [(1,'rama','physics'),(2,'raju','maths'),(1,'rama','maths')]  ,  ('ID','name','subject'))
    results_df=ss.createDataFrame( [(1,'physics',94),(2,'maths',95),(1,'maths',90)]  ,  ('ID','subject','marks_scored'))

##################### DATA SETUP ## END ###################################################################






##################### To see DATAFRAME or its METADATA ## START ###################################################################

1) To see dataframe table
    df1.show(n)                --->>> "n" will be the first number of rows you want to see
    df1.show(n,truncate=False) --->>> To show column data without truncating it to any length
    df1.show(n,truncate=x)     --->>> "x" will be the length of the truncate you need on column values

2) To get schema information of dataframe
    df1.printSchema()

3) To get only columns of dataframe
    df1.columns

##################### To see DATAFRAME or its METADATA ## END ###################################################################






##################### SINGLE DATAFRAME OPERATIONS ## START ###################################################################

###### COULUMN SELECTION

1) SELECT FEW COLUMNS

    1.1) Most used way 

            df1.select("A-col","B-col").show()
            df1.select(df1["A-col"],df1["B-col"]).show()

            df1.select(df1.A-col,df1.B-col).show()       ---->>> Dont use this syntax
            Reason : Even though this  " df1.select(df1.A-col,df1.B-col).show() " works for simple column names ,
                    for A-col it gets confused as if to do A minus col so its better to avoid this syntax
        
    1.2) By using col() function

            from pyspark.sql.functions import col
            df1.select(col("A-col"),col("B-col")).show()

    1.3) select columns by index

            df1.select(df1.columns[2:4]).show(3)


2) CHANGE NAMES OF SELECTED COLUMNS

    df3 = df1.select(df1["A-col"].alias("A") , df1["B-col"].alias("B"))

    2.1) In some cases where you are doing aggregate operations and you want to alias the column header
            which comes as "agg_func(col-c)" , you could use either of the below two ways (preferably second)

            2.1.1) First way :

                            df_temp = df1.groupBy('col-A','col-B').max('col-C').show()
                            df2 = df_temp.select(df_temp['max(col-C)'].alias('your_preferred_name'))

            2.1.2) In case you dont want to create df2 just to rename , and do renaming in one line , then use below :

                            from pyspark.sql.functions import col
                            df1.groupBy('col-A','col-B').max('col-C').select(col('max(col-C)').alias('your-preferred-name')).show()

3) APPLY DISTINCT 

    df1.select(df1['A-col'] , df1['B-col']).distinct().show()



###### COULUMN TYPES AND THEIR CONVERSIONS


###### AGGREGATE FUNCTIONS
Note : Import this module to have these aggregate functions running
       from pyspark.sql.functions import *

1) Sum of values in a column :

    df1.groupBy(df1['col-A'],df1['col-B']).sum('col-c','col-d').show()

2) count of values in a column :

    df1.groupBy(df1['col-A'],df1['col-B']).count().show()

3) Minimum and Maximum of a column

    df1.groupBy(df1['col-A'],df1['col-B']).min('col-c').show()
    df1.groupBy(df1['col-A'],df1['col-B']).max('col-c').show()


###### WINDOW FUNCTIONS
Problem statement :: when you are doing aggregate operations you will be able to only retrive the columns 
                     on which groupby was performed and the aggregate function column .
                     But in case if you also want to display some other non group by columns you would have to 
                     go through the hastle of joining resultant dataframe with source data frame and then obtain
                     those non groupby columns 

Solution given by WINDOW functions :: Even though you are selecting multiple columns , it lets you decide on 
                                      which window of columns you want to apply your aggregate functions.
                                      and also since we have other columns along with group by columns , the output
                                      will come with all the number of rows as previous to this operation


1) Row number over certain window :

    
                    





###### IF ELSE in DATAFRAME DATA




###### FILTER DATA

##################### SINGLE DATAFRAME OPERATIONS  ## END ###################################################################






##################### JOINS ## START ###################################################################

###### INNER JOIN




###### LEFT JOIN




###### RIGHT JOIN




###### OUTER JOIN




###### CROSS JOIN


##################### JOINS ## END ###################################################################
