############ Install PYSPARK and find out where the library is installed in  ## START ###################################################################

1) Install pyspark using terminal
    pip install pyspark 

2) In case you want to see code inside the library and want to open its installed locations , run below python code
    import pyspark
    fp=pyspark.__file__
    print(fp)

3) Find out the "pyspark WEBUI" URL for your sparksessions

    After your sparkContext or SparkSession is running , execute below command inside your python terminal
        from pyspark.sql import SparkSession
        ss=SparkSession.builder.getOrCreate()
        ss.sparkContext.uiWebUrl

############ Install PYSPARK and find out where the library is installed in ## END ###################################################################






##################### DATA SETUP ## START ###################################################################
 
    from pyspark.sql import SparkSession

    ss=SparkSession.builder.getOrCreate()
    students_df=ss.createDataFrame( [(1,'rama','physics'),(2,'raju','maths'),(1,'rama','maths')]  ,  ('ID','name','subject'))
    results_df=ss.createDataFrame( [(1,'physics',94),(2,'maths',95),(1,'maths',90)]  ,  ('ID','subject','marks_scored'))

##################### DATA SETUP ## END ###################################################################






##################### To see DATAFRAME or its METADATA ## START ###################################################################

1) To see dataframe table
    df1.show(n)                --->>> "n" will be the first number of rows you want to see
    df1.show(n,truncate=False) --->>> To show column data without truncating it to any length
    df1.show(n,truncate=x)     --->>> "x" will be the length of the truncate you need on column values

2) To get schema information of dataframe
    df1.printSchema()

3) To get only columns of dataframe
    df1.columns

##################### To see DATAFRAME or its METADATA ## END ###################################################################






##################### SINGLE DATAFRAME OPERATIONS ## START ###################################################################

###### COULUMN SELECTION

    1) SELECT FEW COLUMNS

        1.1) Most used way 

                df1.select("A-col","B-col").show()
                df1.select(df1["A-col"],df1["B-col"]).show()

                df1.select(df1.A-col,df1.B-col).show()       ---->>> Dont use this syntax
                Reason : Even though this  " df1.select(df1.A-col,df1.B-col).show() " works for simple column names ,
                        for A-col it gets confused as if to do A minus col so its better to avoid this syntax
            
        1.2) By using col() function

                from pyspark.sql.functions import col
                df1.select(col("A-col"),col("B-col")).show()

        1.3) select columns by index

                df1.select(df1.columns[2:4]).show(3)


    2) CHANGE NAMES OF SELECTED COLUMNS

        df3 = df1.select(df1["A-col"].alias("A"), df1["B-col"].alias("B"))


    3) APPLY DISTINCT 

        df1.select(df1['A-col'] , df1['B-col']).distinct().show()



###### COULUMN TYPES AND THEIR CONVERSIONS


###### AGGREGATE FUNCTIONS
1) Sum of values in a column :

    df1.groupBy(df1['col-A'],df1['col-B']).sum('col-c','col-d').show()



###### IF ELSE in DATAFRAME DATA




###### FILTER DATA

##################### SINGLE DATAFRAME OPERATIONS  ## END ###################################################################






##################### JOINS ## START ###################################################################

###### INNER JOIN




###### LEFT JOIN




###### RIGHT JOIN




###### OUTER JOIN




###### CROSS JOIN


##################### JOINS ## END ###################################################################
